# -*- coding: utf-8 -*-
"""roof_properties.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/129OaRehp10pBNsvtyxnNpqO8RzQyrLiC

## Contextualization
Roof Imóveis(Fictitious) is one of the largest companies in the Brazilian real estate sector.
and wants to expand its area of operation by making an international investment, with that, she hired you, for a strategic consultancy. <p>
The company will invest in real estate in County County, United States. <p>
Using the available dataset, you will apply the CRISP DM concepts learned so far in a practical project, which seeks to develop your analytical thinking, insights and decision making.

## King County - King Washington County
King County is one of 39 counties in the US state of
Washington. <p>
The county's seat and most populous city is Seattle.
It was founded in 1852. <p>
With over 2.2 million residents, according to the 2020 national census, it is the most populous county in the state and the 12th most populous in the country. <p>
Wikipedia The Dataset presents the sale of properties and their characteristics in the region from May 2014 to May 2015. <p>
It has a total of 21613 records with a total of 21 attributes. <p>

Dataset:
https://www.kaggle.com/harlfoxem/housesalesprediction  <p>
Detailed Dataset:
https://geodacenter.github.io/data <p>
and
lab/KingCounty HouseSales2015/
"""

# Libs for Modeling and Matrices
import numpy as np
import pandas as pd

# Libs for graphical analysis
import matplotlib.pyplot as plt
import seaborn as sns

# Lib to ignore warnings
import warnings

# Disabling warnings
warnings.filterwarnings('ignore')

# Code to import the files
# from google.colab import files 
# upload = files.upload()

"""## Transforming the date fields of the kc_house_data table"""

data = pd.read_csv('kc_house_data.csv')
data['date'] = data['date'].apply(lambda x: x.rstrip('T000000'))
data['date'] = pd.to_datetime(data['date'], format='%Y%m%d') 
data['yr_built'] = pd.to_datetime(data['date'], format='%Y%m%d') 
data['yr_renovated'] = pd.to_datetime(data['date'], format='%Y%m%d') 
data.head()

"""## Read US Zipcode Table for Merge"""

zips = pd.read_excel('uszips.xlsx')
zips.head()

"""## Calculation of the Price by Area of each Property"""

data['sqft_price'] = data['price']/data['sqft_living']
data

"""## Renamed zip column to zipcode in zip table for future merge"""

zips.rename(columns = {'zip':'zipcode'}, inplace = True)
zips

"""## Excluded columns that will not be used in the merge between kc_house_data and zips"""

zips.drop(['lat','lng','zcta','parent_zcta','population','density'	,'county_fips',		'county_names_all','county_fips_all','imprecise','military','timezone','county_weights'], axis=1, inplace=True)
zips

"""## Creating a table that brings the city of each property"""

data_merged = pd.merge(data, zips, on = 'zipcode', how = 'left')
data_merged

"""## Calculating the average price graph by area by city from the database"""

sqft_pricepercity = pd.DataFrame(data_merged.groupby(by ='city')['sqft_price'].mean())
sqft_pricepercity.reset_index(inplace = True)
sqft_pricepercity
plt.figure(figsize=(10,10))
sns.barplot(data = sqft_pricepercity, x = 'city', y='sqft_price', order = sqft_pricepercity.sort_values('sqft_price', ascending=False).city, color = 'magenta', edgecolor = 'black')
plt.xticks(rotation=45, ha='right')
plt.show()

"""## Describe to understand the data"""

data_merged.describe(include = 'all')

"""## Creating the View"""

# Create the figure and subplots
fig = plt.figure(figsize = (10,10))
plt.subplots_adjust(hspace = 0.3, wspace = 0.4)
fig.suptitle('Comparação de distribuições', x=0.1, y=.95, horizontalalignment = 'left', verticalalignment = 'top')
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
ax4 = fig.add_subplot(2, 2, 4)

# Create the histogram plots and adjust the axis names
# Histogram of Sales Quantity by Number of Prices
ax1.hist(x = data_merged['bathrooms'], bins = 8)
ax1.set_xlabel('# of Bathrooms')
ax1.set_ylabel('Frequency')

ax2.hist(x = data_merged['bedrooms'], bins = 33)
ax2.set_xlabel('# of Bedrooms')
ax2.set_ylabel('Frequency')

ax3.hist(x = data_merged['sqft_living'], bins = 20)
ax3.set_xlabel('Living_Area')
ax3.set_ylabel('Frequency')

ax4.hist(x = data_merged['price'], bins = 20)
ax4.set_xlabel('Price')
ax4.set_ylabel('Frequency')

"""## After analyzing the histograms above, the conditions of the properties that we will analyze for purchase were considered"""

condicion_1 = (data_merged['bedrooms'] >= 3) & (data_merged['bedrooms'] < 6)
condicion_2 = (data_merged['bathrooms'] >= 2) & (data_merged['bedrooms'] < 4)
condicion_3 = (data_merged['sqft_living'] >= 1750) & (data_merged['sqft_living'] < 2800)
condicion_4 = (data_merged['price'] <= 1000000)

"""## Creation of a new dataframe, which selects only properties with the conditions declared above"""

data_filtered_v1 = data_merged[condicion_1 & condicion_2 & condicion_3 & condicion_4]
data_filtered_v1

"""## Visualization of cities from the remaining database to select only cities within a radius of up to 15km from the city of Seattle (assumption defined in the text)"""

unique_cities = np.sort(pd.unique(data_filtered_v1['city']))
unique_cities

"""## Condition that selects only cities within 15 km of Seattle"""

condicion_5 = (data_merged['city'] == 'Bellevue') | (data_merged['city'] == 'Kirkland') | (data_merged['city'] == 'Medina') | (data_merged['city'] == 'Mercer Island') | (data_merged['city'] == 'Renton') | (data_merged['city'] == 'Seattle')

"""## Selecting properties within a predetermined 15 km radius of Seattle"""

data_filtered_v2 = data_filtered_v1[condicion_5]
data_filtered_v2

"""## Viewing all table columns with describe() command"""

pd.set_option("max_rows", None)
pd.set_option("max_columns", None)
data_filtered_v2.describe(include = 'all')

data_filtered_v2.sort_values('grade', ascending = False)

"""## Removing properties with Vista grade 0"""

condicion_6 = data_filtered_v2['view'] != 0
data_filtered_v3 = data_filtered_v2[condicion_6]
data_filtered_v3

"""## Averaging the view, condition, grade and price by area"""

data_filtered_v3['view_average'] = data_filtered_v3['view'].mean()
data_filtered_v3['condition_average'] = data_filtered_v3['condition'].mean()
data_filtered_v3['grade_average'] = data_filtered_v3['grade'].mean()
data_filtered_v3['sqft_price_average'] = data_filtered_v3['sqft_price'].mean()

data_filtered_v3

"""## Calculating standard deviations of view, condition, grade and price by area"""

data_filtered_v3['view_std'] = data_filtered_v3['view'].std()
data_filtered_v3['condition_std'] = data_filtered_v3['condition'].std()
data_filtered_v3['grade_std'] = data_filtered_v3['grade'].std()
data_filtered_v3['sqft_price_std'] = data_filtered_v3['sqft_price'].std()

data_filtered_v3

"""## Calculating the z-score of the view, condition, grade and price per area of each property"""

data_filtered_v3['view_zscore'] = (data_filtered_v3['view'] - data_filtered_v3['view_average'])/data_filtered_v3['view_std']
data_filtered_v3['condition_zscore'] = (data_filtered_v3['condition'] - data_filtered_v3['condition_average'])/data_filtered_v3['condition_std']
data_filtered_v3['grade_zscore'] = (data_filtered_v3['grade'] - data_filtered_v3['grade_average'])/data_filtered_v3['grade_std']
data_filtered_v3['sqft_price_zscore'] = (data_filtered_v3['sqft_price'] - data_filtered_v3['sqft_price_average'])/data_filtered_v3['sqft_price_std']

data_filtered_v3

"""## Summing the qualitative z-scores and ordering them"""

data_filtered_v3['quality_zscore'] = data_filtered_v3['view_zscore'] + data_filtered_v3['condition_zscore'] + data_filtered_v3['grade_zscore']
data_filtered_v3.sort_values('quality_zscore', ascending = False)

"""## Calculating the difference between the z-scores and ordering them from highest to lowest"""

data_filtered_v3['zscore_delta'] = data_filtered_v3['quality_zscore'] - data_filtered_v3['sqft_price_zscore']
data_filtered_v3.sort_values('zscore_delta', ascending = False)

"""## Downloading csv for final view"""

data_filtered_v3.sort_values('zscore_delta', ascending = False).to_csv('data_filtered_v3.csv')